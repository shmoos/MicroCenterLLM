
--- Source: Sample-QA ---
Question: What's the difference between DDR4 and DDR5 RAM?

Answer: DDR5 RAM offers higher speeds (up to 6400MHz+) compared to DDR4 (typically 2133-3200MHz). DDR5 also has improved power efficiency with lower voltage (1.1V vs 1.2V) and higher bandwidth. However, DDR5 typically has higher latency and is more expensive. For most gaming and everyday use, DDR4 is still excellent value.



--- Source: Sample-QA ---
Question: How much power supply wattage do I need for an RTX 4090?

Answer: NVIDIA recommends an 850W power supply for the RTX 4090, but many users opt for 1000W or higher for headroom and efficiency. The 4090 has a 450W TDP, so you need to account for your CPU (100-250W), motherboard, drives, and other components. A quality 1000W PSU from brands like Corsair, EVGA, or Seasonic is recommended.



--- Source: Sample-QA ---
Question: Is 16GB RAM enough for gaming in 2024?

Answer: 16GB RAM is still adequate for most gaming in 2024, but 32GB is becoming the recommended amount for new builds. Some modern games like Star Citizen, Cyberpunk 2077, and flight simulators can use more than 16GB. If you multitask (gaming while streaming/browsing), 32GB provides much better headroom. DDR4 16GB kits are affordable, making 32GB a good value upgrade.



--- Source: Sample-QA ---
Question: What CPU should I pair with an RTX 4070?

Answer: For an RTX 4070, good CPU pairings include: Intel Core i5-13600K or i5-14600K for excellent gaming performance, AMD Ryzen 5 7600X or 7700X for great value, or Intel Core i7-13700K/i7-14700K if you also do productivity work. Avoid pairing with older CPUs like the i5-10400 or Ryzen 3600 as they may bottleneck the GPU in CPU-intensive games at 1080p.



--- Source: Sample-QA ---
Question: Do I need a CPU cooler or can I use the stock one?

Answer: Intel K-series CPUs and AMD Ryzen 5 7600X and above don't include stock coolers. Intel non-K chips include basic coolers that work but are loud under load. AMD's stock coolers (Wraith) are better but still noisy. For best results, invest in a tower cooler like the Thermalright Peerless Assassin 120 ($35) or Arctic Freezer 34 eSports. High-end CPUs benefit from 280mm+ AIOs.



--- Source: Wikipedia-Graphics_processing_unit ---
Article: Graphics processing unit

Agraphics processing unit(GPU) is a specializedelectronic circuitdesigned fordigital image processingand to acceleratecomputer graphics, being present either as a component on a discretegraphics cardor embedded onmotherboards,mobile phones,personal computers,workstations, andgame consoles. GPUs were later found to be useful for non-graphic calculations involvingembarrassingly parallelproblems due to theirparallel structure. The ability of GPUs to rapidly perform vast numbers of calculations has led to their adoption in diverse fields includingartificial intelligence(AI) where they excel at handling data-intensive and computationally demanding tasks. Other non-graphical uses include the training ofneural networksandcryptocurrency mining.

Arcade system boardshave used specialized graphics circuits since the 1970s. In early video game hardware,RAMfor frame buffers was expensive, so video chips composited data together as the display was being scanned out on the monitor.[1]

A specializedbarrel shiftercircuit helped the CPU animate theframebuffergraphics for various 1970sarcade video gamesfromMidwayandTaito, such asGun Fight(1975),Sea Wolf(1976), andSpace Invaders(1978).[2]TheNamco Galaxianarcade system in 1979 used specializedgraphics hardwarethat supportedRGB color, multi-colored sprites, andtilemapbackgrounds.[3]The Galaxian hardware was widely used during thegolden age of arcade video games, by game companies such asNamco,Centuri,Gremlin,Irem,Konami, Midway,Nichibutsu,Sega, and Taito.[4]

TheAtari 2600in 1977 used a video shifter called theTelevision Interface Adaptor.[5]Atari 8-bit computers(1979) hadANTIC, a video processor which interpreted instructions describing a "display list"—the way the scan lines map to specificbitmappedor character modes and where the memory is stored (so there did not need to be a contiguous frame buffer).[clarification needed][6]6502machine codesubroutinescould be triggered onscan linesby setting a bit on a display list instruction.[clarification needed][7]ANTIC also supported smoothverticalandhorizontal scrollingindependent of the CPU.[8]

TheNEC μPD7220was the first implementation of apersonal computergraphics display processor as a singlelarge-scale integration(LSI)integrated circuitchip. This enabled the design of low-cost, high-performance video graphics cards such as those fromNumber Nine Visual Technology. It became the best-known GPU until the mid-1980s.[9]It was the first fully integratedVLSI(very large-scale integration)metal–oxide–semiconductor(NMOS) graphics display processor for PCs, supported up to1024×1024 resolution, and laid the foundations for the PC graphics market. It was used in a number of graphics cards and was licensed for clones such as the Intel 82720, the first ofIntel's graphics processing units.[10]TheWilliams Electronicsarcade gamesRobotron: 2084,Joust,Sinistar, andBubbles, all released in 1982, contain customblitterchips for operating on 16-color bitmaps.[11][12]

In 1984,Hitachireleased the ARTC HD63484, the first majorCMOSgraphics processor for personal computers. The ARTC could display up to4K resolutionwhen inmonochromemode. It was used in a number of graphics cards and terminals during the late 1980s.[13]In 1985, theAmigawas released with a custom graphics chip including a blitter for bitmap manipulation, line drawing, and area fill. It also included acoprocessorwith its own simple instruction set, that was capable of manipulating graphics hardware registers in sync with the video beam (e.g. for per-scanline palette switches,sprite multiplexing, and hardware windowing), or driving the blitter. In 1986,Texas Instrumentsreleased theTMS34010, the first fully programmable graphics processor.[14]It could run general-purpose code but also had a graphics-oriented instruction set. During 1990–1992, this chip became the basis of theTexas Instruments Graphics Architecture("TIGA")Windows acceleratorcards.

In 1987, theIBM 8514graphics system was released. It was one of the first video cards forIBM PC compatiblesthat implementedfixed-function2D primitives inelectronic hardware.Sharp'sX68000, released in 1987, used a custom graphics chipset[15]with a 65,536 color palette and hardware support for sprites, scrolling, and multiple playfields.[16]It served as a development machine forCapcom'sCP Systemarcade board. Fujitsu'sFM Townscomputer, released in 1989, had support for a 16,777,216 color palette.[17]In 1988, the first dedicatedpolygonal 3Dgraphics boards were introduced in arcades with theNamco System 21[18]andTaitoAir System.[19]

IBMintroduced itsproprietaryVideo Graphics Array(VGA) display standard in 1987, with a maximum resolution of 640×480 pixels. In November 1988,NEC Home Electronicsannounced its creation of theVideo Electronics Standards Association(VESA) to develop and promote aSuper VGA(SVGA)computer display standardas a successor to VGA. Super VGA enabledgraphics display resolutionsup to 800×600pixels, a 56% increase.[20]

In 1991,S3 Graphicsintroduced theS3 86C911, which its designers named after thePorsche 911as an indication of the performance increase it promised.[21]The 86C911 spawned a variety of imitators: by 1995, all major PC graphics chip makers had added2Dacceleration support to their chips.[22]Fixed-functionWindows acceleratorssurpassed expensive general-purpose graphics coprocessors in Windows performance, and such coprocessors faded from the PC market.

In the early- and mid-1990s,real-time3D graphics became increasingly common in arcade, computer, and console games, which led to increasing public demand for hardware-accelerated 3D graphics. Early examples of mass-market 3D graphics hardware can be found in arcade system boards such as theSega Model 1,Namco System 22, andSega Model 2, and thefifth-generation video game consolessuch as theSaturn,PlayStation, andNintendo 64. Arcade systems such as the Sega Model 2 andSGIOnyx-based Namco Magic Edge Hornet Simulator in 1993 were capable of hardware T&L (transform, clipping, and lighting) years before appearing in consumer graphics cards.[23][24]Another early example is theSuper FXchip, aRISC-basedon-cartridge graphics chipused in someSNESgames, notablyDoomandStar Fox. Some systems usedDSPsto accelerate transformations.Fujitsu, which worked on the Sega Model 2 arcade system,[25]began working on integrating T&L into a singleLSIsolution for use in home computers in 1995;[26]the Fujitsu Pinolite, the first 3D geometry processor for personal computers, announced in 1997.[27]The first hardware T&L GPU onhomevideo game consoleswas theNintendo 64'sReality Coprocessor, released in 1996.[28]In 1997,Mitsubishireleased the3Dpro/2MP, a GPU capable of transformation and lighting, forworkstationsandWindows NTdesktops;[29]ATiused it for itsFireGL 4000graphics card, released in 1997.[30]


--- Source: Wikipedia-Central_processing_unit ---
Article: Central processing unit

Acentral processing unit(CPU), also called acentral processor,main processor, or justprocessor, is the primaryprocessorin a givencomputer.[1][2]Itselectronic circuitryexecutesinstructionsof acomputer program, such asarithmetic, logic, controlling, andinput/output(I/O) operations.[3][4][5]This role contrasts with that of external components, such asmain memoryand I/O circuitry,[6]and specializedcoprocessorssuch asgraphics processing units(GPUs).

The form,design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged.[7]Principal components of a CPU include thearithmetic–logic unit(ALU) that performsarithmeticandlogic operations,processor registersthat supplyoperandsto the ALU and store the results of ALU operations, and acontrol unitthat orchestrates thefetching (from memory),decodingandexecution (of instructions)by directing the coordinated operations of the ALU, registers, and other components. Modern CPUs devote a lot of semiconductor area tocachesandinstruction-level parallelismto increase performance and toCPU modesto supportoperating systemsandvirtualization.

Most modern CPUs are implemented onintegrated circuitmicroprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are calledmulti-core processors(MCP).[8]The individual physical CPUs, calledprocessor cores, can also bemultithreadedto support CPU-level multithreading.[9]

An IC that contains a CPU may also containmemory,peripheralinterfaces, and other components of a computer;[10]such integrated devices are variously calledmicrocontrollersorsystems on a chip(SoC).

Early computers such as theENIAChad to be physically rewired to perform different tasks, which caused these machines to be called "fixed-program computers".[11]The "central processing unit" term has been in use since as early as 1955.[12][13]Since the term "CPU" is generally defined as a device forsoftware(computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of thestored-program computer.

The idea of a stored-program computer had already been present in the design ofJohn Presper EckertandJohn William Mauchly'sENIAC, but was initially omitted so that it could be finished sooner.[14]On June 30, 1945, before ENIAC was made, mathematicianJohn von Neumanndistributed a paper entitledFirst Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949.[15]EDVACwas designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speedcomputer memoryrather than specified by the physical wiring of the computer.[16]This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task.[17]With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC was not the first stored-program computer; theManchester Baby, which was a small-scale experimental stored-program computer, ran its first program on 21 June 1948[18]and theManchester Mark 1ran its first program during the night of 16–17 June 1949.[19]

Early CPUs were custom designs used as part of a larger and sometimes distinctive computer.[20]However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discretetransistormainframesandminicomputers, and has rapidly accelerated with the popularization of theintegrated circuit(IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order ofnanometers.[21]Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles[22]to cellphones,[23]and sometimes even in toys.[24][25]

While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as thevon Neumann architecture, others before him, such asKonrad Zuse, had suggested and implemented similar ideas.[26]The so-calledHarvard architectureof theHarvard Mark I, which was completed before EDVAC,[27][28]also used a stored-program design usingpunched paper taperather than electronic memory.[29]The key difference between the two is that Harvard architecture separates the storage and treatment of CPU instructions and data, whereas von Neumann architecture uses the same memory space for both.[30]Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, theAtmel AVRmicrocontrollers are Harvard-architecture processors.[31]

Prior to the invention of the transistor,relaysandvacuum tubes(thermionic tubes) were commonly used as switching elements;[32][33]a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches.Vacuum-tube computerssuch as EDVAC tended to average eight hours between failures, whereas relay computers—such as the slower but earlierHarvard Mark I—failed very rarely.[13]In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at lowclock ratescompared to modern microelectronic designs. Clock signal frequencies ranging from 100kHzto 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.[34]

The design complexity of CPUs increased as various technologies facilitated the building of smaller and more reliable electronic devices. The first such improvement came with the advent of thetransistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements, likevacuum tubesandrelays.[35]With this improvement, more complex and reliable CPUs were built onto one or severalprinted circuit boardscontaining discrete (individual) components.


--- Source: Wikipedia-Random-access_memory ---
Article: Random-access memory

Random-access memory(RAM;/ræm/) is a form ofelectronic computer memorythat can be read and changed in any order, typically used to store workingdataandmachine code.[1][2]Arandom-accessmemory device allows data items to bereador written in almost the same amount of time irrespective of the physical location of data inside the memory, in contrast with other direct-access data storage media (such ashard disksandmagnetic tape), where the time required to read and write data items varies significantly depending on their physical locations on the recording medium, due to mechanical limitations such as media rotation speeds and arm movement.

In modern technology, random-access memory takes the form ofintegrated circuit(IC) chips withMOS(metal–oxide–semiconductor)memory cells. RAM is normally associated withvolatiletypes of memory where stored information is lost if power is removed. The two main types of volatile random-accesssemiconductor memoryarestatic random-access memory(SRAM) anddynamic random-access memory(DRAM).

Non-volatile RAM has also been developed[3]and other types ofnon-volatile memoriesallow random access for read operations, but either do not allow write operations or have other kinds of limitations. These include most types ofROMandNOR flash memory.

The use of semiconductor RAM dates back to 1965 when IBM introduced the monolithic (single-chip) 16-bit SP95 SRAM chip for theirSystem/360 Model 95computer, andToshibaused bipolar DRAM memory cells for its 180-bit Toscal BC-1411electronic calculator, both based onbipolar transistors. While it offered higher speeds thanmagnetic-core memory, bipolar DRAM could not compete with the lower price of the then-dominant magnetic-core memory.[4]In 1966, Dr.Robert Dennardinvented modern DRAM architecture in which there's a single MOS transistor per capacitor.[5]The first commercial DRAM IC chip, the 1KIntel 1103, was introduced in October 1970.Synchronous dynamic random-access memory(SDRAM) was reintroduced with theSamsungKM48SL2000 chip in 1992.

Early computers usedrelays,mechanical counters[6]ordelay linesfor main memory functions. Ultrasonic delay lines wereserial deviceswhich could only reproduce data in the order it was written.Drum memorycould be expanded at relatively low cost but efficient retrieval of memory items requires knowledge of the physical layout of the drum to optimize speed. Latches built out oftriode vacuum tubes, and later, out ofdiscrete transistors, were used for smaller and faster memories such asregisters. Such registers were relatively large and too costly to use for large amounts of data; generally, only a few dozen or few hundredbitsof such memory could be provided.

The first practical form of random-access memory was theWilliams tube. It stored data as electrically charged spots on the face of acathode-ray tube. Since the electron beam of the CRT could read and write the spots on the tube in any order, memory was random access. The capacity of the Williams tube was a few hundred to around a thousand bits, but it was much smaller, faster, and more power-efficient than using individual vacuum tube latches. Developed at theUniversity of Manchesterin England, the Williams tube provided the medium on which the first electronically stored program was implemented in theManchester Babycomputer, which first successfully ran a program on 21 June, 1948.[7]In fact, rather than the Williams tube memory being designed for the Baby, the Baby was atestbedto demonstrate the reliability of the memory.[8][9]

Magnetic-core memorywas invented in 1947 and developed up until the mid-1970s. It became a widespread form of random-access memory, relying on an array of magnetized rings. By changing the sense of each ring's magnetization, data could be stored with one bit stored per ring. Since every ring had a combination of address wires to select and read or write it, access to any memory location in any sequence was possible. Magnetic core memory was the standard form ofcomputer memoryuntil displaced bysemiconductor memoryinintegrated circuits(ICs) during the early 1970s.[10]

Prior to the development of integratedread-only memory(ROM) circuits,permanent(orread-only) random-access memory was often constructed usingdiode matricesdriven byaddress decoders, or specially woundcore rope memoryplanes.[citation needed]

Semiconductor memoryappeared in the 1960s with bipolar memory, which usedbipolar transistors. Although it was faster, it could not compete with the lower price of magnetic core memory.[11]


--- Source: Wikipedia-Solid-state_drive ---
Article: Solid-state drive

Asolid-state drive(SSD) is a type ofsolid-state storagedevice that usesintegrated circuitsto store datapersistently. It is sometimes calledsemiconductor storage device,solid-state device, orsolid-state disk.[1][2]

SSDs rely on non-volatile memory, typicallyNAND flash, to store data in memory cells. The performance and endurance of SSDs vary depending on the number of bits stored per cell, ranging from high-performing single-level cells (SLC) to more affordable but slower quad-level cells (QLC). In addition to flash-based SSDs, other technologies such as3D XPointoffer faster speeds and higher endurance through different data storage mechanisms.

Unlike traditionalhard disk drives(HDDs), SSDs have no moving parts, allowing them to deliver faster data access speeds, reduced latency, increased resistance to physical shock, lower power consumption, and silent operation.

Often interfaced to a system in the same way as HDDs, SSDs are used in a variety of devices, includingpersonal computers,enterprise servers, andmobile devices. However, SSDs are generally more expensive on a per-gigabyte basis and have a finite number of write cycles, which can lead to data loss over time. Despite these limitations, SSDs are increasingly replacing HDDs, especially in performance-critical applications and as primary storage in many consumer devices.

SSDs come in various form factors and interface types, includingSATA,PCIe, andNVMe, each offering different levels of performance. Hybrid storage solutions, such assolid-state hybrid drives(SSHDs), combine SSD and HDD technologies to offer improved performance at a lower cost than pure SSDs.

An SSD stores data insemiconductorcells, with its properties varying according to the number ofbitsstored in each cell (between 1 and 4). Single-level cells (SLC) store one bit of data per cell and provide higher performance and endurance. In contrast, multi-level cells (MLC), triple-level cells (TLC), and quad-level cells (QLC) store more data per cell but have lower performance and endurance. SSDs using3D XPointtechnology, such as Intel's Optane, store data by changing electrical resistance instead of storing electrical charges in cells, which can provide faster speeds and longer data persistence compared to conventional flash memory.[3]SSDs based onNAND flashslowly leak charge when not powered, while heavily used consumer drives may start losing data typically after one to two years unpowered in storage.[4]SSDs have a limited lifetime number of writes, and also slow down as they reach their full storage capacity.[citation needed]

SSDs also have internal parallelism that allows them to manage multiple operations simultaneously, which enhances their performance.[5]

Unlike HDDs and similarelectromechanicalmagnetic storage, SSDs do not have moving mechanical parts, which provides advantages such as resistance to physical shock, quieter operation, and faster access times. Their lower latency results in higher input/output rates (IOPS) than HDDs.[6]

Some SSDs are combined with traditional hard drives in hybrid configurations, such as Intel'sHystorand Apple'sFusion Drive. These drives use both flash memory and spinning magnetic disks in order to improve the performance of frequently accessed data.[7][8]


--- Source: Wikipedia-Computer_cooling ---
Article: Computer cooling

Computer coolingis required to remove thewaste heatproduced bycomputer hardware, to keep components within permissibleoperating temperaturelimits. Components that are susceptible to temporary malfunction or permanent failure if overheated includeintegrated circuitssuch ascentral processing units(CPUs),chipsets,graphics cards,hard disk drives, andsolid state drives(SSDs).

Components are often designed to generate as little heat as possible, and computers and operating systems may be designed to reduce power consumption and consequent heating according to workload, but more heat may still be produced than can be removed without attention to cooling. Use ofheatsinkscooled by airflow reduces the temperature rise produced by a given amount of heat. Attention to patterns of airflow can prevent the development of hotspots.Computer fansare widely used along with heatsink fans to reduce temperature by actively exhausting hot air. There are also other cooling techniques, such asliquid cooling. All modern day processors are designed to cut out or reduce their voltage or clock speed if the internal temperature of the processor exceeds a specified limit. This is generally known as Thermal Throttling in the case of reduction of clock speeds, or Thermal Shutdown in the case of a complete shutdown of the device or system.

Cooling may be designed to reduce the ambient temperature within the case of a computer, such as by exhausting hot air, or to cool a single component or small area (spot cooling). Components commonly individually cooled include the CPU,graphics processing unit(GPU) and thenorthbridge.

Integrated circuits(e.g. CPU and GPU) are the main generators of heat in modern computers. Heat generation can be reduced by efficient design and selection of operating parameters such as voltage and frequency, but ultimately, acceptable performance can often only be achieved by managing significant heat generation.

In operation, the temperature of a computer's components will rise until the heat transferred to the surroundings is equal to the heat produced by the component, that is, whenthermal equilibriumis reached. For reliable operation, the temperature must never exceed a specified maximum permissible value unique to each component. For semiconductors, instantaneousjunction temperature, rather than component case, heatsink, or ambient temperature is critical.

Because high temperatures can significantly reduce life span or cause permanent damage to components, and the heat output of components can sometimes exceed the computer's cooling capacity, manufacturers often take additional precautions to ensure that temperatures remain within safe limits. A computer withthermal sensorsintegrated in the CPU, motherboard, chipset, or GPU can shut itself down when high temperatures are detected to prevent permanent damage, although this may not completely guarantee long-term safe operation. Before an overheating component reaches this point, it may be "throttled" until temperatures fall below a safe point usingdynamic frequency scalingtechnology. Throttling reduces the operating frequency and voltage of an integrated circuit or disables non-essential features of the chip to reduce heat output, often at the cost of slightly or significantly reduced performance. For desktop and notebook computers, throttling is often controlled at theBIOSlevel. Throttling is also commonly used to manage temperatures in smartphones and tablets, where components are packed tightly together with little to no active cooling, and with additional heat transferred from the hand of the user.[1]

The user can also perform several tasks in order to preemptively prevent damage from happening. They can perform a visual inspection of the cooler and case fans. If any of them are not spinning correctly, it is likely that they will need to be replaced. The user should also clean the fans thoroughly, since dust and debris can increase the ambient case temperature and impact fan performance. The best way to do so is with compressed air in an open space. Another preemptive technique to prevent damage is to replace the thermal paste regularly.[2]

As electronic computers became larger and more complex, cooling of the active components became a critical factor for reliable operation. Early vacuum-tube computers, with relatively large cabinets, could rely on natural or forced air circulation for cooling. However, solid-state devices were packed much more densely and had lower allowable operating temperatures.


--- Source: Wikipedia-Motherboard ---
Article: Motherboard

Amotherboard, also called amainboard, asystem board, alogic board, and informally amobo(see"Nomenclature" section), is the mainprinted circuit board(PCB) ingeneral-purpose computersand other expandable systems. It holds and allows communication between many of the crucial electronic components of a system, such as thecentral processing unit(CPU) andmemory, and provides connectors for otherperipherals.

Unlike abackplane, a motherboard usually contains significant sub-systems, such as the CPU, thechipset'sinput/outputandmemory controllers,interfaceconnectors, and other components integrated for general use.[1]: 48

Oxford English Dictionarytraces the origin of the wordmotherboardto 1965, its earliest-found attestation occurring in the magazineElectronics.[2]The term alludes to its importance and size compared to the components attached to it, being the "mother of all boards" in a computer system.[3]

Several alternative terms formotherboardhave been used in technical documentation and industry practice, includingmainboard,system board,logic board,baseboard, and the informalmobo. These terms are functionally synonymous and reflect regional, corporate, or contextual preferences rather than a coordinated effort to adopt gender-neutral language.[citation needed]

System boardwas used byIBMin documentation for theIBM PCand its derivatives; however, higher-end models in thePS/2line, such as theModel 80, used the termplanarinstead.Applecommonly useslogic boardin its technical documentation for products such as theApple IIand theMac.Inteltypically usesbaseboardin its technical manuals, though it also usesmotherboardinterchangeably.[1]The termmobois an informal truncation ofmotherboard, popularized by computer enthusiasts and builders in the 1990s.[4]

The termmainboardsometimes describes a device with a single board and no additional expansions or capability, such as controlling boards inlaser printers,televisionsets,washing machines,mobile phones, and otherembedded systemswith limited expansion abilities.[citation needed]

Before the advent of themicroprocessor, thecentral processing unit(CPU) of acomputerwas typically implemented using multipleprinted circuit boardshoused in a card cage, interconnected via abackplane—a board containing sockets into which the individual circuit boards were inserted. Early systems used discrete copper wiring between connector pins, but printed circuit boards quickly became the standard. The CPU,main memory, andperipheralcomponents were each located on separate boards connected through the backplane.

With the rise of microprocessors, CPU functionality and supporting circuitry were consolidated onto a single board, while memory and peripherals remained on separate expansion cards plugged into the backplane. A prominent example is theS-100 bus, widely used in 1970s microcomputer systems such as theAltair 8800.

In the 1980s, popular personal computers like theApple IIandIBM Personal Computerfeatured publicly available schematic diagrams and technical documentation. This openness enabled rapidreverse engineeringand the development of third-party motherboards. These clone and upgrade boards often provided enhanced performance or additional features, and were commonly used to modernize or replace original manufacturer hardware.


--- Source: Wikipedia-Power_supply_unit_(computer) ---
Article: Power supply unit (computer)

Apower supply unit(PSU) convertsmains ACto low-voltage regulatedDC powerfor the internal components of adesktop computer. Modern personal computers universally useswitched-mode power supplies. Somepower supplieshave a manual switch for selecting input voltage, while others automatically adapt to the main voltage.

Most modern desktop personal computer power supplies conform to theATX specification, which includes form factor and voltage tolerances. While an ATX power supply is connected to the mains supply, it always provides a 5-voltstandby (5VSB) power so that the standby functions on the computer and certain peripherals are powered. ATX power supplies are turned on and off by a signal from themotherboard. They also provide a signal to the motherboard to indicate when the DC voltages are in spec, so that the computer is able to safely power up and boot. The most recent ATX PSU standard is version 3.1 as of mid 2025.[1]

The desktop computer power supply converts thealternating current(AC) from awall socketofmains electricityto a low-voltagedirect current(DC) to operate the motherboard, processor and peripheral devices. Several direct-current voltages are required, and they must be regulated with some accuracy to provide stable operation of the computer. Apower supply railorvoltage railrefers to a single voltage provided by a PSU.[2]

Some PSUs can also supply astandby voltage, so that most of thecomputersystem can be powered off after preparing for hibernation or shutdown, and powered back on by an event. Standby power allows a computer to be started remotely viawake-on-LANandWake-on-ringor locally via Keyboard Power ON (KBPO) if the motherboard supports it. This standby voltage may be generated by a small linear power supply inside the unit or a switching power supply, sharing some components with the main unit to save cost and energy.

First-generationmicrocomputerandhome computerpower supply units used a heavy step-downtransformerand a linear power supply, as used, in for example, theCommodore PETintroduced in 1977. TheApple II, also introduced in 1977, was noted for itsswitched-mode power supply, which was lighter and smaller than an equivalent linear power supply would have been, and which had no cooling fan.  The switched-mode supply uses aferrite-coredhigh frequencytransformer and power transistors that switch thousands of times per second. By adjusting the switching time of the transistor, the output voltage can be closely controlled without dissipating energy as heat in a linear regulator. The development of high-power and high-voltage transistors at economical prices made it practical to introduce switched-mode supplies that had been used in aerospace, mainframes, minicomputers and color television, into desktop personal computers. TheApple IIdesign byAtariengineerRod Holtwas awarded a patent,[3][4]and was in the vanguard of modern computer power supply design. Now all modern computers use switched-mode power supplies, which are lighter, less costly, and more efficient than equivalent linear power supplies.

Computer power supplies may have short circuit protection, overpower (overload) protection, over-voltage protection, under-voltage protection, over-current protection, and over-temperature protection.

Power supplies designed for worldwide use were once equipped with an input voltage selector switch that allowed the user to configure the unit for use on local power grid. In the lower voltage range, around 115 V, this switch is turned on changing the power grid voltage rectifier into a voltage doubler inDelon circuitdesign. As a result, the large primaryfilter capacitorbehind that rectifier was split up into two capacitors wired in series, balanced withbleeder resistorsandvaristorsthat were necessary in the upper input voltage range, around 230 V. Connecting the unit configured for the lower range to a higher-voltage grid usually resulted in immediate permanent damage. When apower-factor correction(PFC) was required, those filter capacitors were replaced with higher-capacity ones, together with a coil installed in series to delay the inrush current. This is the simple design of a passive PFC.

Active PFC is more complex and can achieve higher PF, up to 99%. The first active PFC circuits just delayed the inrush. Newer ones work as an input and output condition-controlled step-up converter, supplying a single 400 V filter capacitor from a wide-range input source, usually between 80 and 240 V. Newer PFC circuits also replace theNTC-based inrush current limiter, which is an expensive part previously located next to the fuse.

The firstIBM PCpower supply unit (PSU) supplied two main voltages: +5Vand +12 V. It supplied two other voltages, −5 V and −12 V, but with limited amounts of power. Mostmicrochipsof the time operated on 5 V power. Of the 63.5Wthese PSUs could deliver, most of it was on this +5 V rail.

The +12 V supply was used primarily to operate motors such as in disk drives and cooling fans. As more peripherals were added, more power was delivered on the 12 V rail. However, since most of the power is consumed by chips, the 5 V rail still delivered most of the power. The −12 V rail was used primarily to provide the negative supply voltage to theRS-232serial ports. A −5 V rail was provided for peripherals on the ISA bus (such as soundcards), but was not used by any motherboard other than the original IBM PC motherboard.


--- Source: Reddit-buildapc ---
Title: Simple Questions - November 18, 2025

Question/Post: This thread is for simple questions that don't warrant their own thread (although we strongly suggest checking the sidebar and the wiki before posting!). Please don't post involved questions that are better suited to a [[Build Help](https://reddit.com/r/buildapc/search?q=title%3A%22build+ready%22+OR+flair%3A%22build+ready%22&amp;restrict_sr=on&amp;sort=new)], [[Build Ready](https://reddit.com/r/buildapc/search?q=title%3A%22build+ready%22+OR+flair%3A%22build+ready%22&amp;restrict_sr=on&amp;sort=new)] or [[Build Complete](https://reddit.com/r/buildapc/search?q=title%3A%22build+complete%22+OR+flair%3A%22build+complete%22&amp;restrict_sr=on&amp;sort=new)] post.  
Examples of questions suitable for here:

* Is this RAM compatible with my motherboard?
* I'm thinking of getting a ≤$300 graphics card. Which one should I get?
* I'm on a very tight budget and I'm looking for a case ≤$50

**Important: Downvotes are strongly discouraged in this thread. Sorting by new is strongly encouraged.**

Have a question about the subreddit or otherwise for r/buildapc mods? [We welcome your mod mail!](http://www.reddit.com/message/compose?to=%2Fr%2Fbuildapc)

To easily find previous simple questions posts, [use this link.](http://www.reddit.com/r/buildapc/search?q=title%3A%22simple+questions%22+author%3A%22AutoModerator%22&amp;restrict_sr=on&amp;sort=new&amp;t=all)



--- Source: Reddit-buildapc ---
Title: Seasons of RTX - $5000 Dream PC Build Contest with NVIDIA!

Question/Post: Hope everyone has been keeping warm. Fortunately the colder weather opens up the best time to enjoy PC gaming! See below for more details + entry instructions from NVIDIA for your chance at an up to $5000 USD PC of your design grand prize.

\~\~\~

Hey everyone, 

We’re officially kicking off the GeForce Seasons of RTX! From today through January, celebrate the season with some [great deals ](https://www.nvidia.com/en-us/geforce/campaigns/holiday-deals/)on NVIDIA GeForce RTX 50 Series graphics cards, laptops, desktops, G-SYNC® displays, and GeForce NOW. 

https://preview.redd.it/06lo26ld1v0g1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=b956b8816500bf95b4a56cdab14906a0f964bb7a

[**Seasons of RTX**](https://www.youtube.com/watch?v=KDRVDagdVRY)

To mark the Seasons of RTX, we’re inviting the community to take part in a holiday-themed PC build contest.

**To participate, here is what you will need to do:**

1. Use [PCPartPicker](https://pcpartpicker.com/) to create your build. 
2. Include a GeForce RTX 50 Series graphics card.
3. Budget: up to $5,000 USD (including the graphics card)
4. The build should include the following components: CPU, CPU Cooler, Motherboard, Memory, Storage (e.g., SSD), Case, Power Supply, and a full version of the Windows Operating System. Optional additions include a Monitor, Expansion cards, and Accessories (e.g., Fans, Fan Controller, Optical Drive).
5. It may NOT include peripherals (e.g., Headphones, Keyboard, Mice, Speakers, Web Camera)  
6. Optional:
   1. Make it festive; think lights, themes, or just the ultimate rig you’d love to game on during the holidays.
7. Post your PC Part Picker build list in the comments section below. (Instructions for how to select the permalink can be found here: [LINK](https://imgur.com/IW0iaOm))

The contest starts on 11/12 and ends on 11/19.

Whether you’re going for maximum performance, cozy holiday vibes, or just a stunning setup for 2026, we can’t wait to see what you create. 

[*Terms and conditions apply*](https://www.nvidia.com/en-us/geforce/contests/seasonsofrtx-build-contest/)  

\~\~\~

All the best to everyone participating! The contest is open in many regions around the world but confirm your locale's eligibility in the [Terms &amp; Conditions](https://www.nvidia.com/en-us/geforce/contests/seasonsofrtx-build-contest/).

Contest window: November 12 2025 @ 12:30PM PT to November 19 2025 @ 5PM PT.

One winner will be selected by the NVIDIA team and contacted through Reddit chat (keep an eye out for a PM from u/NV-Randy

Previous contest winners were announced in the previous thread [HERE](https://www.reddit.com/r/buildapc/comments/1mtwlt7/geforce_rtx_pc_build_contest_share_your_dream_rig/)



--- Source: Reddit-buildapc ---
Title: Most logical upgrade from 3060

Question/Post: Hello!
Im brainstorming a future upgrade of my current RTX 3060, I desire it to be the most hassle-free so I don’t have to replace my current PSU. 
Bonus if the upgrade is to an AMD as im thinking of switching to Linux.

Thanks in advance!



--- Source: Reddit-buildapc ---
Title: HELP! How much will a dual core cpu bottleneck an rx 580 8gb?

Question/Post: My rx 580 is performing very badly even though temperature in stress test (76 degrees) and clock speed is normal. I know my cpu (i3 4130) is holding my gpu back but to what extent? I saw some benchmarks on elden ring for this card and its about 50-60 fps on medium settings 1080p but im getting an avg of 25 fps on low settings 720p, will a bottleneck like that limit my gpu that much?

Also, power consumption is pretty low (about 70 w, yes i have connected the 8 pci-e pins to the card) is this because of the bottleneck? What's really confusing me is that in sekiro im getting even worse performance than my last card (gtx 750 2gb) on the same settings even though that card couldnt even load elden ring. Another game i tried was counter strike 2 which crashed when i tried to load on to deathmatch (I still had this problem on my previous gpu) and the loading time is still pretty bad. Dark souls 3 on the other hand is giving me much better performance than the gtx 750.

Idk if you could tell but i dont know that much about gpu+cpu combinations or bottlenecks. I just wanna know if a bottleneck like that can cause that much of a decrease in performance so I can know if its a problem with my gpu which i recently bought so i can return it, so if someone could tell me whats going on, that would be very helpful. 

Edit: it'd help if i could get some recommendations on what cpu to buy for my gpu that would minimize bottleneck as much as possible while also being affordable.





--- Source: Reddit-buildapc ---
Title: How many fans should I have ?

Question/Post: Hi, I’m building a new computer but I’m a bit confused on how many reverse and how many ordinary a need. I’m getting the lian Li 011d mini v2. 
Right now I’m thinking 3 reverse and 3 non reverse + the 360mm watercooler in the top. Am i thinking right ? 





--- Source: Reddit-buildapc ---
Title: What monitor for 5080

Question/Post: Hi guys I have a 9800x3d 5080. Would like to hear which montior you guys running with your 5080.

Thinking about this

https://www.centrecom.com.au/asus-rog-strix-xg32ucg-315-4k-160hz-fhd-320hz-dual-mode-ips-g-sync-gaming-monitor

Or can you suggest something else in this price range.

Must be 10bit over 160hz ips 4k 31.5/32 inch


Would love to hear what you guys have thanks 



--- Source: Reddit-buildapc ---
Title: Is DLSS worth it over Amd?

Question/Post: I don't properly understand how DLSS works and if i had to choose between a 9060 XT and a 5060 Ti, which one should I pick?



--- Source: Reddit-buildapc ---
Title: What do you think about the Intel GPUs?

Question/Post: I'm thinking about getting the Arc B580 12GB because it's cheaper than other options but I've seen bad reviews about it.
I would appreciate the help.



--- Source: Reddit-buildapc ---
Title: help! this is a bunch of gibberish

Question/Post: Im looking at pc part picker, what does this mean?

The MSI B850 GAMING PLUS WIF ATX AM5 Motherboard supports the AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor with BIOS version 7E56v1A2. If the motherboard is using an older BIOS version, updating the BIOS will be necessary to support the CPU



--- Source: Reddit-buildapc ---
Title: How much does RAM MHz for overall PC perfomance

Question/Post: With RAM prices soaring, , the cheapest RAM i can find in my country is DDR5 16GB 4800 MHz from only one place. 
DDR5 16GB 5600 MHz &amp; 6000 MHz is 3x of that price . And DDR5 2×16GB (32 GB) is 40 percent more . 
So how much performance difference does a RAM make to the entire overall performance of a PC . I asked ChatGPT and somewhere it said upto 10 - 15 % performance difference  from 4800 MHz to 6000 MHz . 

How much of is it true. 

Edit ::  My GPU and CPU are both AMD . Apparently this matters when considering RAM speed on PC builds and its overall performance effect. 



--- Source: Reddit-buildapc ---
Title: PC Upgrade Priority to get the most out of current build

Question/Post: Hello everyone, I was wondering what parts I should upgrade to get the most out of my prebuilt PC I bought in Dec 2021 before I do a complete rebuild within the next couple years. Here is my parts list:

-----
iBUYPOWER Trace 7 Mesh Pro ARGB Gaming Case

Gigabyte GeForce RTX 3060 Ti Eagle OC 8G

NEO FORZA 16GB (8GB x 2) DDR4 Ram U-DIMM 3000 1.35V CL15 NMUD480E85-3000DG00

TeamGroup 2.5" Vulcan G 512GB SATA III 6Gb/s SSD t253tg512g3c301 Solid State Drv

Seagate BarraCuda 1TB Internal Hard Drive HDD – 3.5 Inch SATA 6 Gb/s 7200 RPM 64MB Cache for Computer Desktop PC (ST1000DM010)

HIGH POWER 600W Power Supply, 80 Plus Gold Certified PSU, Continuous Power with 120mm Whisper Quiet Fan, ATX 12V V2.3/EPS 12V Active PFC Reliable Performance, Energy-Efficient

GIGABYTE B560M DS3H AC (LGA 1200/ Intel/ B560/ Micro-ATX/Dual M.2/ PCIe 4.0/ USB 3.2 Gen1/ GbE LAN/Motherboard)

Deepcool Theta 20 PWM CPU Cooler
$10


Intel® Core™ i7-11700F Desktop Processor 8 Cores up to 4.9 GHz LGA1200 (Intel® 500 Series &amp; Select 400 Series chipset) 65W
-----

I'm planning to get a M.2 nvme 2TB SSD for faster loading speeds in games. I'm mostly wondering if I should a better cooler and/or upgraded case fans. I was also wondering if I should get another 16 GB kit of the same RAM specs (3000 CL 15) or get a whole new 32 GB 3600 MHz CL16 RAM kit. Thank you for your time



--- Source: Reddit-buildapc ---
Title: Can anyone see any issues with this build? Also MOBO choice?

Question/Post: Are there any issues with this build and I dont know whether to go with Gigabyte B850 Aorus Elite WIFI7 or a MSI MAG B850 Tomahawk Max WiFi?

[https://uk.pcpartpicker.com/list/zN4vTM](https://uk.pcpartpicker.com/list/zN4vTM)



--- Source: Reddit-buildapc ---
Title: Opinion on this 7800X3D + RX 9070XT build for 1440p/4K gaming?

Question/Post: I’ve put together this build and wanted to get some feedback before I go ahead with it.
My main use is 1440p gaming (possibly some 4K later), no streaming or heavy productivity.
Looking for thoughts on part compatibility, bottlenecks, and whether anything should be swapped.

Build (local market prices):

- **CPU**: AMD Ryzen 7 7800X3D – *$460*

- **GPU**: Gigabyte RX 9070XT Gaming OC 16GB – *$926*

- **Motherboard**: ASUS TUF B650M-Plus Gaming WiFi – *$255*

- **Cooler**: Thermalright Peerless Assassin 120 SE ARGB – *$40*

- **RAM**: Patriot Viper Venom DDR5 6000MHz (2 × 16GB sticks) – *$100 each*

- **PSU**: Lian Li Edge Gold 1000W – *$168*

- **Case**: Lian Li Lancool 216RX – *$100*

- **SSD**: Colorful CN700 SE 1TB PCIe 4.0 (7000/6000 MB/s) – *$85*


One of my hesitations is the 5070Ti. The *
*Colorful Battle-AX* version (the cheapest one) is $100 extra, but every comparison I’ve watched makes the performance difference look too small to be worth it.



--- Source: Reddit-buildapc ---
Title: Upgrade CPU fo 4k?

Question/Post: Currently running 3080ti with a i5 12600k raptor lake on an LG C2 OLED 120hz 42"

Mainly playing BF6 on high-ultra 4k and no dlss and surprisingly getting smooth frames for the most part, although I do get some stutter in heavy action. I'd like to go to overkill graphics if I can and get high frames still

Considering upgrading to a 5080. However I keep reading that CPU doesn't matter as much in 4k?

Wondering if I would be wasting my money if I got a 9800x3d to pair with it? Or if keeping my 3080 and adding the 9800x3d would improve my frames and allow me to go overkill settings without having to grab a 5080 as well



--- Source: Reddit-buildapc ---
Title: is my combination good?

Question/Post: check, my combination of ryzen 5 8500g and rtx 4060 8b is good ?, more than anything because I notice that it has 50% spikes (my processor) when I play minecraft and transmit screen with task manager open i have those spikes when i load chunks (i play in 15 chunks with create mod) (100Hz and 1080p)



the point is that my gpu does nothing while my processor is killing itself, and I don't know if it's normal



I don't know if it's for something in particular



I wanted to put together my pc so I wouldn't have to change anything to him like 2 years minimum, and seeing that makes me wonder if it can be something in particular



--- Source: Reddit-buildapc ---
Title: New PC build! 9070xt - 9800x3d

Question/Post: After a couple weeks of browsing this sub, I finally caved and built my dream rig. Coming from an AMD 5600xt - ryzen 5 3600 build I did a little over 5 years ago, this is a serious upgrade!

New build:

CPU: 9800x3d

Mobo: Asus B650E-E tuf gaming 

Memory: 32gb DDR5 6000 cl36

GPU: ASrock 9070xt

PSU: be quiet 13m 1000w

I went ahead and bought the microcenter bundle of the 9800x3d and 650E. Was a pretty sweet deal! The RAM isn’t cl30, but from what I gather, the difference is negligible. I will post pics of the complete setup, still waiting on a few more parts!

Question: I’ve read about updating the Bios for the 650E. That being said, out of the box, manufacturing date is in 2025 and 9000 series is printed right on the box. Is it recommended to update this prior to putting it all together? I’m of the opinion to try and see if it takes and update if not.



--- Source: Reddit-buildapc ---
Title: Would it be wise to purchase a GPU now before prices go up? Currently running a 3070ti.

Question/Post: My 3070ti is starting to show some wear.  1/20 sessions I get distortions for a frame or two every minute or so and need to restart.  My current gaming requirements are on a 4k and certainly I can't game Cyberpunk or BF6 at max settings.  But I also have a backlog of games that the 3070ti can handle.  

I have put some money aside for a new GPU and looking at the 5070ti to possibly replace it.  I can probably put it off another year or so, but I've been building rigs for 20  years now.  If that 5070 goes up to $1,000 next year and stays there until 2027, I'd be better off to purchase something this holiday season.  What are your thoughts on getting an upgrade now with the hopes of avoiding a 40% price hike.



--- Source: Reddit-buildapc ---
Title: Don’t have the right power cables for gpu ?

Question/Post: To start. I’ll be honest I’m a pretty noobie builder so I can barely parse the info about different connectors online


Here’s my psu https://www.microcenter.com/product/695332/CORE_GX-850_850_Watt_80_Plus_Gold_ATX_Fully_Modular_Power_Supply_-_ATX_31_Compatible?

I got the cpu and mobo cables connected and those power on just fine, but when I tried connecting this cable that I think is 12v to a pigtail with 2x 8 pin connectors, the gpu wouldn’t power on 

So I looked up and saw that it’s recommended to use 2 separate pcie cables

But all I have is one this pigtail one, a single pcie cable, and a cpu/pcie cable that wouldn’t fit (I tried) 

Before trying to get a different psu or seeing if my gpu is dead, is there something I’m missing with the cables ?



--- Source: Reddit-buildapc ---
Title: Affordable upgrade path from my 2017 build (Windows 10 out of support, CPU bottleneck)

Question/Post: Hey everyone,

I built this PC back in 2017 and it has served me well, but it’s definitely showing its age now. Windows 10 is out of support, I can’t upgrade to Windows 11, and the CPU is a bottleneck, especially in games. I was thinking about simply moving to Linux, but a few games I play with friends do not work with Linux yet.  
I’m looking for advice on a budget-friendly upgrade that also gives me a future-proof platform for later high-end upgrades.

**Current Specs:**

* **CPU:** Intel Core i5-7500 (4 cores / 4 threads, 3.4 GHz)
* **GPU:** NVIDIA GTX 1070 8GB
* **RAM:** 16 GB DDR4 dual-channel @ 2400 MHz
* **Motherboard:** Gigabyte B250-HD3P
* **Storage:** 1 TB HDD + 240 GB SSD + 500 GB NVMe SSD
* **OS:** Windows 10 Home (no Windows 11 support)
* **PSU:** 450 W Cougar GX Series 80+ Gold, \~8 years old

**Goals / Budget:**  
Since I just started earning proper money, I cannot spend too much right now. I was hoping to keep the upgrade in the **€200–€300 range**. I’m not a hardcore gamer anymore—I mainly play casual multiplayer games (LoL, CS, Battlefield, CoD) but also want to run modern singleplayer games from time to time with high-end graphics. In other words, I don’t want to be limited, but I probably won't max out everything to the point that nothing would use the power anyway even when I have the money.

I’m overwhelmed by all the options, so I’d love advice on what combination gives me a reasonable modern starting point without overspending, but still lets me upgrade to high-end components later. My first reasearch pointed me towards a Ryzen 5 7600X. 

Thanks in advance for any recommendations!



--- Source: Reddit-buildapc ---
Title: 9600X or 7800X3D

Question/Post: I mostly play single-player games, sometimes I play some competitive games with my friends but most of the time I'm playing single-player games, so the CPU wont change much, but I also like some simulation games, like ketbal space program,  and I've heard that the x3d CPUs really help performance in those, I've also heard that using upscaling will also use more of your CPU. My GPU is going to be a 9070 xt, would you say it's worth getting the 7800x3d over the 9600x? The 9600x is 195€ and the 7800x3d is 289€. I was thinking of getting the 9600x so I could save a bit money to maybe get an oled monitor, but maybe I can stretch my budget a bit and still get an oled with the 7800x3d.



--- Source: Reddit-buildapc ---
Title: 9800x3D is Ram 6000 CL30 needed?

Question/Post: Ram prices are increasing every week, and this one seems to be the most expensive.

Would a 6400 Cl32 be that bad or other speeds if they are cheaper? I have read that it's not as good and could be unstable unless tweaked.

His much worse will it be if used without tweaking? I keep reading 1:1 is best but how much better is it then not being 1:1? I don't have experience messing with Ram but unless it's a massive difference would it be fine using it out of the box?

Edit: I'm still confused. Is 6400 not stable at all and I'm prone to lots of crashes? How much slower out of the box would it be compared to 6000?



--- Source: Reddit-buildapc ---
Title: Is anyone deciding to upgrade GPU right now based on the RAM shortages/price hikes?

Question/Post: I was already on the fence about upgrading. Yesterday when the report came out that AMD has notified board partners about incoming price increases that was enough for me, bought a 5080 at MSRP that should last me at 4K for awhile. Figured I'd be kicking myself in a month if I absolutely decided on an upgrade and the same card was now selling for $1500.

I'm wondering if anyone else is doing the same? Or deciding to ride it out with what you currently have? I could see price hikes being a reality for the foreseeable future, maybe 1-2 years depending on how long the AI bubble lasts.



--- Source: Reddit-buildapc ---
Title: need help with my pc assembly

Question/Post: hello, last night I had the courage to assemble my PC, I arrived at 80% of my assembly I will say, I have a Thermalright Aqua Elite 360 ​​watercooling, but I am stuck putting the watercooling on I installed it but when it comes time to connect the fan cables I struggle, I would like some help please



--- Source: Reddit-buildapc ---
Title: Considering an Upgrade

Question/Post: Ryzen 7 5700G// Gigabyte RTX 5060ti 8GB Windforce OC (recently purchased) // TeamGroup 8GB 2400MHz 2x // Gigabyte B550M K // ARESGAME AGV Series 650W Power Supply, 80 Plus Bronze Power Supply // 1TB SSD

  
I purchased a 2080ti 11GB since my brother has the same gpu and his system performs much better than mine especially during ARC RAIDERS at 4K.

  
alot of video reviews show that the 2080ti outperforms the 5060ti 8GB so i dont know what do you guys think &gt; 



--- Source: Reddit-buildapc ---
Title: Advice for gpu upgrade please

Question/Post: So my 3060ti is showing signs of death. Yesterday randomly the fanstop led switched on and the GPU crashed ? The monitor turned off saying no signal but the other things were still running. I'm getting some random screen artifacts when opening some games but it disappears after the game is launched. 

I'm currently running on an old setup with i5-10600 16gb ddr4 ram paired with the 3060ti. I've recently bought a 7800x3d with 32gb ddr5 ram and b650 motherboard. I'm undecided on the GPU. I can get a 9070xt gigabyte for $799 or 5070 pny for $779 ~ prices in CAD. Which GPU would you recommend for my new build. Thanks. 



--- Source: Reddit-buildapc ---
Title: How are we feeling about this?

Question/Post: [**https://fi.pcpartpicker.com/list/x82N3w**](https://fi.pcpartpicker.com/list/x82N3w)



--- Source: Reddit-buildapc ---
Title: UPS extension cabel PC

Question/Post: Hello. so i want to get UPS Qoltec 50721 UPS 1000VA 700W

but its a bit noisy a friend has it. 

so i want to put it in a seperate room but the problem is i dont want to run long extension cabel from ups out. what i want to do is to get solid core 11m long wire 3x2.5mm

from ups out ill have flexible cord 20cm that plugs in ups out and to a wall socket that directly goes to my pc. ill have someone that builds the wire in the wall. then from there the pc will plug in that wall socket that goes directly to ups out with 1.8m pc and monitor powercabls.

can you do that will that kinda distance afect how the ups works or can cause some other problems? 





--- Source: Reddit-buildapc ---
Title: Does this build have any problems?

Question/Post: Hi , if anyone is able to help to see if i made any mistakes id be greatly appreciated. 

Im  upgrading mainly to get some blackfriday deals and tarkov.

64gb of ram is only because of tarkov wich can give a 15% increase in lows.

Id mainly like opinions on cooler , motherboard and maybe case ( it has to be arround this size)

https://pt.pcpartpicker.com/list/jxj7zP



--- Source: Reddit-buildapc ---
Title: Still not working..

Question/Post: As I told yesterday, I swapped my gpu from rx6600 to 9060xt 
After that, it died 
Today I swapped the psu and the mainboard, my pc still wont boot 
There is nothing, not even the led or fan is working
I checked everything Like 100 times and I dont know what else to do, I hope that someone can help me with that Problem



--- Source: Reddit-buildapc ---
Title: Q: Are x3D chips always better for strategy/sim games?

Question/Post: Hello

Upgrading desktop and laptop this holiday season. I mostly play RTS (stellaris/EU4), Sims (factorio), Elder Scrolls Online, and civilization titles (although recent ones have been lackluster).

  
I was under the impression that AMDs 3D chips would do better in general for these game categories. However, I watched this video [https://www.youtube.com/watch?v=t9-M4Raf5JI](https://www.youtube.com/watch?v=t9-M4Raf5JI) comparing the laptop CPUs and noticed that Intel did better on civilization turn times. This prompted more investigation. I realize its hard to do good benchmarks on games that have random 'events'/need for user input to progress. Some comments here stated you could fallback on the synthetic benchmarks as a good marker for gaming performance in these genres. However, most of those comments were before the 3D chips started coming out.

  
Further reading it seems like the RTS, simulation, and MMOs benefit from the larger cache but the games with 'AI processing time' bottlenecks like civilization would match better with core clock speed/synthetic benchmarks for workload.  How close am I to getting this right?

  
TIA


